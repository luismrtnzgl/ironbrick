{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lego_retirados = pd.read_csv('../01_Data_Cleaning/df_lego_final_retirados.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtro los sets retirados en el último año\n",
    "df_recent_retirados_last_year = df_lego_retirados[df_lego_retirados[\"YearsSinceExit\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino los temas que queremos filtrar\n",
    "selected_themes = [\n",
    "    \"Speed Champions\", \"Architecture\", \"BrickHeadz\", \"Star Wars\", \"Ideas\", \"Collectable Minifigures\",\n",
    "    \"Technic\", \"Minecraft\", \"Harry Potter\", \"Icons\", \"Ninjago\", \"Education\", \"Jurassic World\", \"DC Comics Super Heroes\", \"Marvel Super Heroes\", \"Creator\", \"City\",\n",
    "    \"Classic\", \"Disney\", \"Creator Expert\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_lego_retirados[(df_lego_retirados[\"YearsSinceExit\"] == 1) & \n",
    "                                (df_lego_retirados[\"Theme\"].isin(selected_themes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de sets por archivo\n",
    "num_sets_por_archivo = 95\n",
    "num_batches = (len(df_filtered) // num_sets_por_archivo) + (1 if len(df_filtered) % num_sets_por_archivo > 0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo y guardo cada archivo\n",
    "arch_part = []\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * num_sets_por_archivo\n",
    "    end_idx = start_idx + num_sets_por_archivo\n",
    "    df_part = df_filtered.iloc[start_idx:end_idx]\n",
    "    \n",
    "    # Guardo cada partición como archivo CSV\n",
    "    nombre_particion = f\"../04_Extra/API_Brickeconomy/lego_scraping_brickeco_{i+1}.csv\"\n",
    "    df_part.to_csv(nombre_particion, index=False)\n",
    "    arch_part.append(nombre_particion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../04_Extra/API_Brickeconomy/lego_scraping_brickeco_1.csv',\n",
       " '../04_Extra/API_Brickeconomy/lego_scraping_brickeco_2.csv',\n",
       " '../04_Extra/API_Brickeconomy/lego_scraping_brickeco_3.csv',\n",
       " '../04_Extra/API_Brickeconomy/lego_scraping_brickeco_4.csv',\n",
       " '../04_Extra/API_Brickeconomy/lego_scraping_brickeco_5.csv']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arch_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 122327: 3.02 USD, 3.65 USD\n",
      "Scraped 122328: 3.52 USD, 3.72 USD\n",
      "Scraped 122329: 2.98 USD, 4.27 USD\n",
      "Scraped 122330: 2.82 USD, 3.98 USD\n",
      "Scraped 122331: 3.16 USD, 3.34 USD\n",
      "Scraped 122332: 3.18 USD, 3.84 USD\n",
      "Scraped 122333: 3.36 USD, 4.06 USD\n",
      "Scraped 122334: 3.93 USD, 5.52 USD\n",
      "Scraped 212325: 2.6 USD, 3.18 USD\n",
      "Scraped 212326: 3.46 USD, 4.0 USD\n",
      "Scraped 212327: 4.22 USD, 5.71 USD\n",
      "Scraped 212328: 3.5 USD, 4.74 USD\n",
      "Scraped 212329: 3.34 USD, 4.85 USD\n",
      "Scraped 212330: 3.91 USD, 4.91 USD\n",
      "Scraped 242316: 3.59 USD, 4.86 USD\n",
      "Scraped 242317: 3.98 USD, 5.59 USD\n",
      "Scraped 242318: 4.47 USD, 5.4 USD\n",
      "Scraped 242319: 4.65 USD, 6.07 USD\n",
      "Error scraping 242320: HTTPSConnectionPool(host='www.brickeconomy.com', port=443): Max retries exceeded with url: /api/v1/set/242320 (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)')))\n",
      "Scraped 242321: 5.49 USD, 6.79 USD\n",
      "Scraped 662302: 4.69 USD, 6.12 USD\n",
      "Scraped 662303: 4.47 USD, 5.17 USD\n",
      "Scraped 662304: 4.44 USD, 6.45 USD\n",
      "Scraped 662305: 5.22 USD, 5.78 USD\n",
      "Scraped 662306: 4.64 USD, 5.36 USD\n",
      "Scraped 662307: 3.57 USD, 4.48 USD\n",
      "Scraped 662308: 4.64 USD, 4.91 USD\n",
      "Scraped 662309: 4.79 USD, 6.25 USD\n",
      "Scraped 662317: 4.47 USD, 6.5 USD\n",
      "Scraped 682303: 5.2 USD, 5.91 USD\n",
      "Scraped 682304: 4.06 USD, 4.89 USD\n",
      "Scraped 682305: 6.16 USD, 7.54 USD\n",
      "Scraped 682306: 3.94 USD, 5.34 USD\n",
      "Scraped 892302: 6.07 USD, 7.47 USD\n",
      "Scraped 892303: 4.74 USD, 6.02 USD\n",
      "Scraped 892304: 7.26 USD, 9.2 USD\n",
      "Scraped 892305: 3.68 USD, 4.62 USD\n",
      "Scraped 892306: 2.8 USD, 3.75 USD\n",
      "Scraped 892307: 6.51 USD, 7.95 USD\n",
      "Scraped 892308: 2.72 USD, 3.88 USD\n",
      "Scraped 892309: 4.15 USD, 5.48 USD\n",
      "Scraped 892310: 3.32 USD, 4.66 USD\n",
      "Scraped 892311: 4.84 USD, 6.39 USD\n",
      "Scraped 892312: 4.47 USD, 6.5 USD\n",
      "Scraped 892313: 3.56 USD, 4.17 USD\n",
      "Scraped 892401: 3.05 USD, 4.13 USD\n",
      "Scraped 912302: 7.28 USD, 8.79 USD\n",
      "Scraped 912303: 8.14 USD, 13.4 USD\n",
      "Scraped 912304: 3.26 USD, 3.98 USD\n",
      "Scraped 912305: 4.86 USD, 5.87 USD\n",
      "Scraped 912306: 3.29 USD, 4.62 USD\n",
      "Scraped 912307: 5.23 USD, 6.54 USD\n",
      "Scraped 912308: 3.3 USD, 4.31 USD\n",
      "Scraped 912309: 6.55 USD, 8.17 USD\n",
      "Scraped 912310: 6.05 USD, 7.29 USD\n",
      "Scraped 912311: 3.86 USD, 5.23 USD\n",
      "Scraped 912312: 2.67 USD, 3.22 USD\n",
      "Scraped 912313: 4.88 USD, 6.12 USD\n",
      "Scraped 912401: 5.47 USD, 7.33 USD\n",
      "Scraped 952302: 2.77 USD, 4.26 USD\n",
      "Scraped 952303: 3.59 USD, 4.4 USD\n",
      "Scraped 952304: 4.22 USD, 5.17 USD\n",
      "Scraped 952305: 3.53 USD, 4.26 USD\n",
      "Scraped 952306: 3.04 USD, 4.27 USD\n",
      "Scraped 952307: 2.42 USD, 3.96 USD\n",
      "Scraped 952308: 3.44 USD, 5.0 USD\n",
      "Scraped 952309: 2.84 USD, 3.87 USD\n",
      "Scraped 952310: 2.63 USD, 3.78 USD\n",
      "Scraped 952311: 3.18 USD, 4.46 USD\n",
      "Scraped 952312: 3.01 USD, 3.33 USD\n",
      "Scraped 952401: 3.84 USD, 5.58 USD\n",
      "Scraped 2000455: 7.87 USD, 8.6 USD\n",
      "Error scraping 2000460: 400 Client Error: Bad Request for url: https://www.brickeconomy.com/api/v1/set/2000460\n",
      "Error scraping 5007872: 400 Client Error: Bad Request for url: https://www.brickeconomy.com/api/v1/set/5007872\n",
      "Scraped 5008076: 50.6 USD, 72.88 USD\n",
      "Scraped 6476267: 22.93 USD, 32.73 USD\n",
      "Scraped 6482525: 16.54 USD, 27.42 USD\n",
      "Scraped 6484261: N/A USD, N/A USD\n",
      "Error scraping 6487483: 400 Client Error: Bad Request for url: https://www.brickeconomy.com/api/v1/set/6487483\n",
      "Scraping completado. Datos guardados en ../04_Extra/API_Brickeconomy/scraped_lego_scraping_brickeco_5.csv. Eliminado lego_scraping_brickeco_5.csv para evitar reuso.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Configuración del entorno\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"BRICKECONOMY_API_KEY\")\n",
    "BASE_URL = \"https://www.brickeconomy.com/api/v1/set/\"\n",
    "HEADERS = {\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"x-apikey\": API_KEY\n",
    "}\n",
    "REQUEST_DELAY = 10  # Espera entre requests para evitar bloqueos\n",
    "MAX_REQUESTS_PER_DAY = 95  # Límite de requests diarios\n",
    "BATCH_FOLDER = \"../04_Extra/API_Brickeconomy/\"\n",
    "\n",
    "# Obtenemos el siguiente archivo de lote disponible\n",
    "def get_next_batch():\n",
    "    batch_files = sorted([f for f in os.listdir(BATCH_FOLDER) if f.startswith(\"lego_scraping_brickeco_\") and f.endswith(\".csv\")])\n",
    "    if batch_files:\n",
    "        return batch_files[0]  # Elegimos siempre el primer archivo disponible\n",
    "    return None\n",
    "\n",
    "batch_filename = get_next_batch()\n",
    "if not batch_filename:\n",
    "    print(\"No hay más archivos de scraping disponibles para descargar de la API.\")\n",
    "    exit()\n",
    "\n",
    "file_path = os.path.join(BATCH_FOLDER, batch_filename)\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# HAcemos una lista para almacenar los resultados\n",
    "scraped_data = []\n",
    "\n",
    "# Limitamos el número de sets a descargar según el límite de la API\n",
    "df = df.head(MAX_REQUESTS_PER_DAY)\n",
    "\n",
    "# Bucle de cada uno de lso sets\n",
    "for index, row in df.iterrows():\n",
    "    set_number = row[\"Number\"]\n",
    "    url = f\"{BASE_URL}{set_number}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        data = response.json().get(\"data\", {})\n",
    "        \n",
    "        # Obtener precios históricos de nuevo y usado\n",
    "        price_events_new = data.get(\"price_events_new\", [])\n",
    "        price_events_used = data.get(\"price_events_used\", [])\n",
    "        \n",
    "        for event in price_events_new:\n",
    "            scraped_data.append({\n",
    "                \"Number\": data.get(\"set_number\", \"N/A\"),\n",
    "                \"SetName\": data.get(\"name\", \"N/A\"),\n",
    "                \"Theme\": data.get(\"theme\", \"N/A\"),\n",
    "                \"Year\": data.get(\"year\", \"N/A\"),\n",
    "                \"Pieces\": data.get(\"pieces_count\", \"N/A\"),\n",
    "                \"Minifigs\": data.get(\"minifigs_count\", \"N/A\"),\n",
    "                \"RetailPriceUSD\": data.get(\"retail_price_us\", \"N/A\"),\n",
    "                \"CurrentValueNew\": data.get(\"current_value_new\", \"N/A\"),\n",
    "                \"ForecastValueNew2Y\": data.get(\"forecast_value_new_2_years\", \"N/A\"),\n",
    "                \"ForecastValueNew5Y\": data.get(\"forecast_value_new_5_years\", \"N/A\"),\n",
    "                \"RollingGrowthLastYear\": data.get(\"rolling_growth_lastyear\", \"N/A\"),\n",
    "                \"RollingGrowth12M\": data.get(\"rolling_growth_12months\", \"N/A\"),\n",
    "                \"PriceType\": \"New\",\n",
    "                \"PriceDate\": event[\"date\"],\n",
    "                \"PriceValue\": event[\"value\"],\n",
    "                \"Currency\": data.get(\"currency\", \"N/A\"),\n",
    "                \"URL\": url\n",
    "            })\n",
    "        \n",
    "        for event in price_events_used:\n",
    "            scraped_data.append({\n",
    "                \"Number\": data.get(\"set_number\", \"N/A\"),\n",
    "                \"SetName\": data.get(\"name\", \"N/A\"),\n",
    "                \"Theme\": data.get(\"theme\", \"N/A\"),\n",
    "                \"Year\": data.get(\"year\", \"N/A\"),\n",
    "                \"Pieces\": data.get(\"pieces_count\", \"N/A\"),\n",
    "                \"Minifigs\": data.get(\"minifigs_count\", \"N/A\"),\n",
    "                \"RetailPriceUSD\": data.get(\"retail_price_us\", \"N/A\"),\n",
    "                \"CurrentValueUsed\": data.get(\"current_value_used\", \"N/A\"),\n",
    "                \"ForecastValueNew2Y\": data.get(\"forecast_value_new_2_years\", \"N/A\"),\n",
    "                \"ForecastValueNew5Y\": data.get(\"forecast_value_new_5_years\", \"N/A\"),\n",
    "                \"RollingGrowthLastYear\": data.get(\"rolling_growth_lastyear\", \"N/A\"),\n",
    "                \"RollingGrowth12M\": data.get(\"rolling_growth_12months\", \"N/A\"),\n",
    "                \"PriceType\": \"Used\",\n",
    "                \"PriceDate\": event[\"date\"],\n",
    "                \"PriceValue\": event[\"value\"],\n",
    "                \"Currency\": data.get(\"currency\", \"N/A\"),\n",
    "                \"URL\": url\n",
    "            })\n",
    "        \n",
    "        print(f\"Scraped {set_number}: {data.get('current_value_new', 'N/A')} USD, {data.get('forecast_value_new_5_years', 'N/A')} USD\")\n",
    "        \n",
    "        time.sleep(REQUEST_DELAY)  # Evitamos los bloqueos con el límite\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error scraping {set_number}: {e}\")\n",
    "\n",
    "# Guardamos los datos en un CSV\n",
    "output_file = os.path.join(BATCH_FOLDER, f\"scraped_{batch_filename}\")\n",
    "pd.DataFrame(scraped_data).to_csv(output_file, index=False)\n",
    "\n",
    "# Eliminar el archivo del scraping procesado para evitar los duplicados\n",
    "os.remove(file_path)\n",
    "print(f\"Scraping completado. Datos guardados en {output_file}. Eliminado {batch_filename} para evitar reuso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiero unificar todos los df de la API en uno solo\n",
    "# Lista de archivos a unificar\n",
    "file_paths = [\n",
    "    \"../04_Extra/API_Brickeconomy/scraped_lego_scraping_brickeco_1.csv\",\n",
    "    \"../04_Extra/API_Brickeconomy/scraped_lego_scraping_brickeco_2.csv\",\n",
    "    \"../04_Extra/API_Brickeconomy/scraped_lego_scraping_brickeco_3.csv\",\n",
    "    \"../04_Extra/API_Brickeconomy/scraped_lego_scraping_brickeco_4.csv\",\n",
    "    \"../04_Extra/API_Brickeconomy/scraped_lego_scraping_brickeco_5.csv\"\n",
    "]\n",
    "\n",
    "# Cargamos y hacemos un concat con los dataframes\n",
    "df_list = [pd.read_csv(file) for file in file_paths]\n",
    "df_concat = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Guardamos el dataframe unificado\n",
    "url_salida = \"../04_Extra/APP/data/scraped_lego_data.csv\"\n",
    "df_concat.to_csv(url_salida, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
